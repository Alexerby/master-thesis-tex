\section{Method} 

MOVE THIS TEXT TO THE LITERATURE REVIEW SECTION

\paragraph{Non take up of welfare.}

We define non take up of welfare in line with Nelson and Nieuwenhuis (2019), as the circumstance when a person is eligible for welfare, but does not receive it. This is in line with terminology commonly used in literature on welfare take up rates. Non take up rate is thus the number of people who are eligible, but do not receive it, divided by the total number of people eligible. It is worth noting, however, that these situations often prove to be more complicated. In some cases, some individuals might receive welfare even though they aren’t eligible. This might for example happen due to fraud or errors made on the administrative level. This presents the issue of type two beta errors \citep{herber_non-take-up_2019, nelson_towards_2021}.

\paragraph{Tertiary non take up of welfare.}
In 2017, Van Mechelen and Janssens defined tertiary non-take-up
as a situation in which vulnerable individuals aren’t entitled to social welfare due to eligibility rules, even
if they are in need of support. Tertiary non-take up as defined here can thus be considered a specific
form of non take up. Originally, this concept was defined narrowly to include only those who are not
eligible within a vulnerable group. However, some have argued for a broader definition that includes
everyone in the vulnerable group that does not actually receive the welfare benefit, regardless of whether
they are eligible or not <\citep{mechelen_who_2017, goedeme_concept_2020}.

The concept of non-tertiary take-up relates directly to the concept of targeting efficiency, which can be
divided into vertical and horizontal efficiency. Vertical efficiency refers to how well a welfare system avoids
giving support to individuals who fall outside the intended target group. In most cases, the target group
is defined as those who are not considered economically vulnerable. It is essentially about minimising
incorrect inclusion. One way to express this is through leakage, which is defined as the proportion of
benefit recipients who are not a part of the reference population. The reference population is typically
defined as people with low living standards, low income or other markers of economic vulnerability.
In contrast, horizontal efficiency focuses on whether those within the target group actually receive the
support. If many eligible or vulnerable individuals go without welfare benefits, the system is horizontally
inefficient. This concept aligns closely with the broader definition of tertiary non-take-up, which includes
all vulnerable individuals who are unable to access support, regardless of the reason \citep{mechelen_who_2017, goedeme_concept_2020}.


\subsection{Overview of Simulation Approach}


\subsection{Pipeline Design}


\subsection{Modelling of non-take-up rate}
After simulating statutory eligibility, we analyse the \textit{behavioural} non-take-up: 
that is, the probability that a student refrains from taking up BAföG even though they are theoretically eligible according to our statutory microsimulation. 
Formally, we model:
\begin{equation}
  \Pr(\mathrm{NTU}_i = 1 \mid \mathbf{X}_i), \quad \text{with } T_i = 1 \text{ for all } i,
\end{equation}
where \( T_i = 1 \) indicates theoretical eligibility, and \( \mathrm{NTU}_i := \mathbf{1}\{A_i = 0\} \) is a binary indicator for non-take-up, derived from the observed take-up variable \( A_i \) in the SOEP-Core dataset.

Due to the binary nature of the dependent variable, we employ a Probit model with a standard normal link function. Specifically, we assume a latent index structure for the unobserved propensity to forego BAföG:
\begin{equation}
  \mathrm{NTU}_i^* = \mathbf{X}_i \beta + \varepsilon_i, 
  \quad 
  \varepsilon_i \sim \mathcal{N}(0,1),
\end{equation}
and observe:
\begin{equation}
  \mathrm{NTU}_i = \mathbf{1} \{ \mathrm{NTU}_i^* > 0 \}.
\end{equation}
That is, \( \mathrm{NTU}_i = 1 \) if and only if the student's latent utility for not applying exceeds zero.

This specification implies the following probability model:
\begin{equation}
  \Pr(\mathrm{NTU}_i = 1 \mid \mathbf{X}_i) = \Pr(\mathrm{NTU}_i^* > 0 \mid \mathbf{X}_i) = \Phi(\mathbf{X}_i \beta),
\end{equation}
where \( \Phi(\cdot) \) denotes the cumulative distribution function (CDF) of the standard normal distribution.

Hence, the latent-index formulation yields the conditional non-take-up probability for each student, and aligns with our empirical target:
\[
\Pr(\mathrm{NTU}_i = 1 \mid \mathbf{X}_i) = \Phi(\mathbf{X}_i \beta), \quad \text{for all } i \text{ such that } T_i = 1.
\]

% TODO: Explain here why we are not using Logit 
% and maybe we also should do that and compare outcomes?

The condition \( T_{i} = 1 \) restricts the dataset to students for whom the microsimulation yields positive statutory entitlement. Therefore, all the inferences pertains to this eligible sub-population whom are \textit{theoretically} (statutorily) eligible. 

The design matrix (\( \mathbf{X} \)) combines three partitions 
\begin{equation}
  \mathbf{X}_{i} = 
  \begin{bmatrix}
    \mathbf{Z}_{i} \mid \mathbf{B}_{i} \mid \mathbf{D}_{i}
  \end{bmatrix}
\end{equation}
where \( \mathbf{Z}_{i} \) (such as \textit{age, number of siblings}) contains continuous covariates, \( \mathbf{B}_{i} \) containing binary indicators (such as \textit{livings with parents})  and \( \mathbf{D}_{i} \) containing dummy vectors (such as \textit{sex, federal state, employment status}). 

The parameters are estimated by maximum likelihood (ML), and inference is conducted using heteroskedasticity-robust standard errors of the \textbf{MacKinnon and White (1985) HC2} type \citep{mackinnon_heteroskedasticity-consistent_1985}. These standard errors account for potential residual heteroskedasticity, which is common in large-scale observational panel data such as the SOEP-Core.\footnote{
We use the HC2 estimator following \citet{mackinnon_heteroskedasticity-consistent_1985}, which adjusts the residual variance by downweighting high-leverage observations. Specifically, the variance-covariance matrix of the estimator is given by:
\(
\widehat{\mathrm{Var}}(\hat{\beta}) = (X^\top X)^{-1} X^\top \operatorname{diag}\left( \frac{e_i^2}{1 - h_{ii}} \right) X (X^\top X)^{-1},
\)
where \( e_i \) are the residuals and \( h_{ii} \) are the diagonal elements of the hat matrix \( H = X(X^\top X)^{-1}X^\top \). This choice is motivated by \citet{chesher_hajek_1989} and \citet[Section 4]{chesher_finite-sample_1991}, who show that HC2 might be better as HC3 tends to be upward biased and HC1 downwards-biased.
}

% What is meant by latent index?
As raw Probit coefficients represents shifts in the latent index\footnote{
With the latent index we refer to the continuous "score" which drives the true/false decision, but is not itself observed. That is the unseen propensity that the model assumes sits underneath every given binary outcome.
}
and are not directly comparable across covariates. To make these comparable, we report the average marginal effects (AMEs) in the results section. This translates the covariate changes into percentage-point differences in the non-take-up probability of the individuals.


